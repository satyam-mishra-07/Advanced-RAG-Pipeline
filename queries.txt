1️⃣ Simple factual retrieval (baseline sanity checks)

These test whether your chunking + dense retrieval works at all.

“What was Microsoft’s total revenue in fiscal year 2023?”

“Who is Microsoft’s CEO as mentioned in the report?”

“What is Microsoft’s mission statement?”

“How many employees did Microsoft have as of June 30, 2023?”

“Which stock exchange is Microsoft listed on?”

If these fail → your basic RAG is broken.

2️⃣ Section-specific queries (tests chunking + metadata)

These require the retriever to find the right section, not just similar words.

“What does Microsoft say about responsible AI?”

“What investments has Microsoft made in sustainability?”

“What is Microsoft Fabric and where is it mentioned?”

“How does Microsoft describe its Intelligent Cloud segment?”

“What is Microsoft’s approach to cybersecurity?”

These expose bad chunk boundaries very quickly.

3️⃣ Multi-sentence reasoning (naive RAG often fails here)

These require combining information across paragraphs.

“How does Microsoft describe the two breakthroughs defining the new era of AI?”

“How is Microsoft integrating AI across productivity tools?”

“What role does Azure play in Microsoft’s AI strategy?”

“How does Microsoft link AI to productivity gains?”

Naive RAG usually retrieves half the answer.

4️⃣ Comparison & decomposition queries (advanced RAG test)

These should fail badly without query decomposition.

“Compare Microsoft’s AI strategy with its cloud strategy.”

“How do GitHub Copilot and Microsoft 365 Copilot differ?”

“What are the differences between Microsoft’s three operating segments?”

“Compare Microsoft’s commitments to sustainability and digital trust.”

These are perfect for:

query decomposition

multi-retrieval

reranking

5️⃣ Negative / trick queries (hallucination tests)

These check whether your system can say “not found”.

“What does the report say about Apple’s revenue?”

“Does the report mention Tesla?”

“What is Microsoft’s plan for cryptocurrency mining?”

“Does Microsoft discuss quantum computing revenue numbers?”

If your system answers confidently → hallucination problem.

6️⃣ Numbers + table-based queries (hard mode)

These test whether your chunks include tables properly.

“How much did Microsoft spend on share repurchases in 2023?”

“What was the dividend per share declared in June 2023?”

“How did Microsoft stock perform compared to the S&P 500 over five years?”

“How much revenue did LinkedIn generate in fiscal year 2023?”

These expose:

PDF extraction issues

table chunking problems

7️⃣ Strategy & narrative queries (semantic understanding)

These test semantic grounding, not keyword matching.

“What does Microsoft mean by ‘Copilot as an everyday AI companion’?”

“How does Microsoft justify its long-term AI investments?”

“What risks does Microsoft associate with AI, and how does it address them?”

“How does Microsoft connect AI to social responsibility?”

BM25 alone will struggle here.

8️⃣ Queries you SHOULD save for advanced RAG only

Do not expect naive RAG to answer these well.

“Summarize Microsoft’s AI vision in 5 bullet points.”

“What are Microsoft’s top priorities for the next platform shift?”

“How does Microsoft plan to balance AI innovation and regulation?”

“What evidence does Microsoft provide that AI is already improving productivity?”

These require:

reranking

context compression

good prompt constraints

9️⃣ Gold-standard evaluation queries (use these repeatedly)

These are perfect for regression testing.

“What are Microsoft’s three commitments for building AI responsibly?”

“How does Microsoft define the new era of AI?”

“What role does natural language play in Microsoft’s AI vision?”

If answers improve after adding:

reranker

better chunking

hybrid retrieval
you’re doing things right.