RAG Evaluation Query Set for Microsoft Annual Report

This document defines a structured set of evaluation queries designed to test and benchmark Retrieval-Augmented Generation (RAG) systems using Microsoft’s annual report as the source corpus. The queries are grouped by difficulty and failure mode to expose weaknesses in retrieval, chunking, reasoning, and hallucination control.

1. Simple Factual Retrieval (Baseline Sanity Checks)



These queries verify whether basic chunking and dense retrieval are functioning correctly.

What was Microsoft’s total revenue in fiscal year 2023?

Who is Microsoft’s CEO as mentioned in the report?

What is Microsoft’s mission statement?

How many employees did Microsoft have as of June 30, 2023?

Which stock exchange is Microsoft listed on?


Failure on these queries indicates a fundamental issue with the RAG pipeline.


---

2. Section-Specific Queries (Chunking and Metadata Validation)



These queries require retrieval from the correct document section rather than simple keyword similarity.

What does Microsoft say about responsible AI?

What investments has Microsoft made in sustainability?

What is Microsoft Fabric and where is it mentioned?

How does Microsoft describe its Intelligent Cloud segment?

What is Microsoft’s approach to cybersecurity?


These queries quickly expose poor chunk boundaries or missing metadata.


---

3. Multi-Sentence Reasoning Queries (Naive RAG Failure Cases)



These queries require combining information across multiple paragraphs.

How does Microsoft describe the two breakthroughs defining the new era of AI?

How is Microsoft integrating AI across productivity tools?

What role does Azure play in Microsoft’s AI strategy?

How does Microsoft link AI to productivity gains?


Naive RAG systems typically retrieve incomplete context for these questions.


---

4. Comparison and Decomposition Queries (Advanced RAG Evaluation)



These queries generally require query decomposition and multiple retrieval steps.

Compare Microsoft’s AI strategy with its cloud strategy.

How do GitHub Copilot and Microsoft 365 Copilot differ?

What are the differences between Microsoft’s three operating segments?

Compare Microsoft’s commitments to sustainability and digital trust.


These queries benefit from multi-retrieval, reranking, and structured reasoning.


---

5. Negative and Trick Queries (Hallucination Detection)



These queries test whether the system can correctly respond when information is absent.

What does the report say about Apple’s revenue?

Does the report mention Tesla?

What is Microsoft’s plan for cryptocurrency mining?

Does Microsoft discuss quantum computing revenue numbers?


Confident answers to these queries indicate hallucination issues.


---

6. Numerical and Table-Based Queries (Hard Mode)



These queries depend on correct extraction and chunking of tables and numerical data.

How much did Microsoft spend on share repurchases in 2023?

What was the dividend per share declared in June 2023?

How did Microsoft stock perform compared to the S&P 500 over five years?

How much revenue did LinkedIn generate in fiscal year 2023?


These queries expose PDF parsing errors and table-handling limitations.


---

7. Strategy and Narrative Queries (Semantic Understanding)



These queries evaluate semantic grounding beyond keyword matching.

What does Microsoft mean by “Copilot as an everyday AI companion”?

How does Microsoft justify its long-term AI investments?

What risks does Microsoft associate with AI, and how does it address them?

How does Microsoft connect AI to social responsibility?


Lexical retrieval methods alone typically perform poorly on these queries.


---

8. Queries Reserved for Advanced RAG Systems



These queries should not be expected to perform well under naive RAG setups.

Summarize Microsoft’s AI vision in five bullet points.

What are Microsoft’s top priorities for the next platform shift?

How does Microsoft plan to balance AI innovation and regulation?

What evidence does Microsoft provide that AI is already improving productivity?


These queries require reranking, context compression, and strong prompt constraints.


---

9. Gold-Standard Evaluation Queries (Regression Testing)



These queries are suitable for repeated evaluation after system improvements.

What are Microsoft’s three commitments for building AI responsibly?

How does Microsoft define the new era of AI?

What role does natural language play in Microsoft’s AI vision?


Improved performance on these queries after enhancements such as reranking, better chunking, or hybrid retrieval indicates meaningful system progress.
